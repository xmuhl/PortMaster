#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PortMasterç»¼åˆæµ‹è¯•è¿è¡Œå™¨
æ‰§è¡Œå®Œæ•´çš„è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ç­‰
"""

import os
import sys
import json
import time
import subprocess
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Windowsç¼–ç è®¾ç½®
if sys.platform == 'win32':
    subprocess.run(['chcp', '65001'], shell=True, capture_output=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class ComprehensiveTestRunner:
    """ç»¼åˆæµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.start_time = datetime.now()
        self.test_results = []
        self.current_phase = ""
        self.report_dir = Path("test_reports")
        self.report_dir.mkdir(exist_ok=True)

    def log_phase(self, phase_name: str):
        """è®°å½•æµ‹è¯•é˜¶æ®µ"""
        self.current_phase = phase_name
        logging.info(f"=== {phase_name} ===")

    def run_build_verification(self) -> Dict:
        """è¿è¡Œæ„å»ºéªŒè¯"""
        self.log_phase("æ„å»ºéªŒè¯")

        try:
            result = subprocess.run(
                ["autobuild_x86_debug.bat"],
                capture_output=True,
                text=True,
                timeout=300
            )

            success = result.returncode == 0

            # æ£€æŸ¥ç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶
            exe_exists = os.path.exists("build/Debug/PortMaster.exe")

            build_result = {
                "phase": "æ„å»ºéªŒè¯",
                "success": success and exe_exists,
                "duration": time.time(),
                "details": {
                    "build_returncode": result.returncode,
                    "exe_exists": exe_exists,
                    "stdout_lines": len(result.stdout.split('\n')) if result.stdout else 0
                }
            }

            if build_result["success"]:
                logging.info("âœ“ æ„å»ºéªŒè¯é€šè¿‡")
            else:
                logging.error("âœ— æ„å»ºéªŒè¯å¤±è´¥")

            self.test_results.append(build_result)
            return build_result

        except subprocess.TimeoutExpired:
            error_result = {
                "phase": "æ„å»ºéªŒè¯",
                "success": False,
                "duration": 300,
                "error": "æ„å»ºè¶…æ—¶"
            }
            logging.error("âœ— æ„å»ºéªŒè¯è¶…æ—¶")
            self.test_results.append(error_result)
            return error_result

    def run_unit_tests(self) -> Dict:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        self.log_phase("å•å…ƒæµ‹è¯•")

        try:
            # ä½¿ç”¨ä¿®å¤åçš„è‡ªåŠ¨åŒ–ç³»ç»Ÿ
            from autotest_integration import AutoTestIntegration

            autotest = AutoTestIntegration()
            if not autotest.verify_autotest_ready():
                raise Exception("AutoTestæœªå°±ç»ª")

            # è¿è¡Œå•å…ƒæµ‹è¯•
            test_result = autotest.run_unit_tests("unit_tests_report.json")

            unit_test_result = {
                "phase": "å•å…ƒæµ‹è¯•",
                "success": test_result.get("success", False),
                "duration": time.time(),
                "details": {
                    "exit_code": test_result.get("exit_code", -1),
                    "report_path": test_result.get("report_path"),
                    "analysis": test_result.get("analysis", {})
                }
            }

            if unit_test_result["success"]:
                analysis = test_result.get("analysis", {})
                logging.info(f"âœ“ å•å…ƒæµ‹è¯•é€šè¿‡ ({analysis.get('passed_tests', 0)}/{analysis.get('total_tests', 0)})")
            else:
                logging.error("âœ— å•å…ƒæµ‹è¯•å¤±è´¥")

            self.test_results.append(unit_test_result)
            return unit_test_result

        except Exception as e:
            error_result = {
                "phase": "å•å…ƒæµ‹è¯•",
                "success": False,
                "duration": time.time(),
                "error": str(e)
            }
            logging.error(f"âœ— å•å…ƒæµ‹è¯•å¼‚å¸¸: {e}")
            self.test_results.append(error_result)
            return error_result

    def run_integration_tests(self) -> Dict:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        self.log_phase("é›†æˆæµ‹è¯•")

        try:
            from autotest_integration import AutoTestIntegration

            autotest = AutoTestIntegration()
            test_result = autotest.run_integration_tests("integration_tests_report.json")

            integration_result = {
                "phase": "é›†æˆæµ‹è¯•",
                "success": test_result.get("success", False),
                "duration": time.time(),
                "details": {
                    "exit_code": test_result.get("exit_code", -1),
                    "report_path": test_result.get("report_path"),
                    "analysis": test_result.get("analysis", {})
                }
            }

            if integration_result["success"]:
                analysis = test_result.get("analysis", {})
                logging.info(f"âœ“ é›†æˆæµ‹è¯•é€šè¿‡ ({analysis.get('passed_tests', 0)}/{analysis.get('total_tests', 0)})")
            else:
                logging.error("âœ— é›†æˆæµ‹è¯•å¤±è´¥")

            self.test_results.append(integration_result)
            return integration_result

        except Exception as e:
            error_result = {
                "phase": "é›†æˆæµ‹è¯•",
                "success": False,
                "duration": time.time(),
                "error": str(e)
            }
            logging.error(f"âœ— é›†æˆæµ‹è¯•å¼‚å¸¸: {e}")
            self.test_results.append(error_result)
            return error_result

    def run_performance_tests(self) -> Dict:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        self.log_phase("æ€§èƒ½æµ‹è¯•")

        try:
            from autotest_integration import AutoTestIntegration

            autotest = AutoTestIntegration()
            test_result = autotest.run_performance_tests("performance_tests_report.json")

            performance_result = {
                "phase": "æ€§èƒ½æµ‹è¯•",
                "success": test_result.get("success", False),
                "duration": time.time(),
                "details": {
                    "exit_code": test_result.get("exit_code", -1),
                    "report_path": test_result.get("report_path"),
                    "analysis": test_result.get("analysis", {})
                }
            }

            if performance_result["success"]:
                analysis = test_result.get("analysis", {})
                logging.info(f"âœ“ æ€§èƒ½æµ‹è¯•é€šè¿‡ ({analysis.get('passed_tests', 0)}/{analysis.get('total_tests', 0)})")
            else:
                logging.error("âœ— æ€§èƒ½æµ‹è¯•å¤±è´¥")

            self.test_results.append(performance_result)
            return performance_result

        except Exception as e:
            error_result = {
                "phase": "æ€§èƒ½æµ‹è¯•",
                "success": False,
                "duration": time.time(),
                "error": str(e)
            }
            logging.error(f"âœ— æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")
            self.test_results.append(error_result)
            return error_result

    def run_functionality_verification(self) -> Dict:
        """è¿è¡ŒåŠŸèƒ½éªŒè¯"""
        self.log_phase("åŠŸèƒ½éªŒè¯")

        try:
            # è¿è¡ŒåŠŸèƒ½éªŒè¯è„šæœ¬
            result = subprocess.run(
                ["python", "test_portmaster_functionality.py"],
                capture_output=True,
                text=True,
                timeout=120
            )

            success = result.returncode == 0

            # è¯»å–åŠŸèƒ½éªŒè¯æŠ¥å‘Š
            func_report = {}
            if os.path.exists("portmaster_functionality_report.json"):
                with open("portmaster_functionality_report.json", "r", encoding="utf-8") as f:
                    func_report = json.load(f)

            functionality_result = {
                "phase": "åŠŸèƒ½éªŒè¯",
                "success": success,
                "duration": time.time(),
                "details": {
                    "returncode": result.returncode,
                    "functionality_report": func_report
                }
            }

            if functionality_result["success"]:
                success_rate = func_report.get("success_rate", 0)
                logging.info(f"âœ“ åŠŸèƒ½éªŒè¯é€šè¿‡ (æˆåŠŸç‡: {success_rate}%)")
            else:
                logging.error("âœ— åŠŸèƒ½éªŒè¯å¤±è´¥")

            self.test_results.append(functionality_result)
            return functionality_result

        except Exception as e:
            error_result = {
                "phase": "åŠŸèƒ½éªŒè¯",
                "success": False,
                "duration": time.time(),
                "error": str(e)
            }
            logging.error(f"âœ— åŠŸèƒ½éªŒè¯å¼‚å¸¸: {e}")
            self.test_results.append(error_result)
            return error_result

    def generate_comprehensive_report(self) -> Dict:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        end_time = datetime.now()
        total_duration = end_time - self.start_time

        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.get("success", False))
        failed_tests = total_tests - passed_tests
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        report = {
            "test_summary": {
                "test_start": self.start_time.strftime("%Y-%m-%d %H:%M:%S"),
                "test_end": end_time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_duration": str(total_duration),
                "total_phases": total_tests,
                "passed_phases": passed_tests,
                "failed_phases": failed_tests,
                "success_rate": success_rate
            },
            "test_results": self.test_results,
            "recommendations": []
        }

        # ç”Ÿæˆå»ºè®®
        if success_rate >= 90:
            report["recommendations"].append("ç³»ç»Ÿæµ‹è¯•é€šè¿‡ç‡è‰¯å¥½ï¼Œå¯ä»¥è¿›å…¥ç”Ÿäº§ç¯å¢ƒ")
        if success_rate < 100:
            report["recommendations"].append("å­˜åœ¨å¤±è´¥çš„æµ‹è¯•é˜¶æ®µï¼Œéœ€è¦æ£€æŸ¥å’Œä¿®å¤")
        if failed_tests > 0:
            report["recommendations"].append("å»ºè®®ä¿®å¤å¤±è´¥çš„é—®é¢˜åé‡æ–°è¿è¡Œæµ‹è¯•")

        return report

    def save_report(self, report: Dict):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        # ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š
        json_file = self.report_dir / f"comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
        md_file = self.report_dir / f"comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.generate_markdown_report(report, md_file)

        logging.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        logging.info(f"  JSON: {json_file}")
        logging.info(f"  Markdown: {md_file}")

    def generate_markdown_report(self, report: Dict, output_file: Path):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        md_content = f"""# PortMaster ç»¼åˆæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è¿°

- **æµ‹è¯•å¼€å§‹æ—¶é—´**: {report['test_summary']['test_start']}
- **æµ‹è¯•ç»“æŸæ—¶é—´**: {report['test_summary']['test_end']}
- **æ€»æµ‹è¯•æ—¶é•¿**: {report['test_summary']['total_duration']}
- **æµ‹è¯•é˜¶æ®µæ•°**: {report['test_summary']['total_phases']}
- **é€šè¿‡é˜¶æ®µæ•°**: {report['test_summary']['passed_phases']}
- **å¤±è´¥é˜¶æ®µæ•°**: {report['test_summary']['failed_phases']}
- **æ€»ä½“é€šè¿‡ç‡**: {report['test_summary']['success_rate']:.1f}%

## æµ‹è¯•ç»“æœè¯¦æƒ…

"""

        for result in report['test_results']:
            status = "âœ… é€šè¿‡" if result.get('success', False) else "âŒ å¤±è´¥"
            md_content += f"### {result['phase']}\n\n"
            md_content += f"- **çŠ¶æ€**: {status}\n"
            md_content += f"- **æ‰§è¡Œæ—¶é—´**: {result['duration']:.2f}ç§’\n"

            if 'error' in result:
                md_content += f"- **é”™è¯¯ä¿¡æ¯**: {result['error']}\n"

            if 'details' in result:
                details = result['details']
                if 'analysis' in details and details['analysis']:
                    analysis = details['analysis']
                    md_content += f"- **æµ‹è¯•ç»Ÿè®¡**: æ€»è®¡{analysis.get('total_tests', 0)}ä¸ª, é€šè¿‡{analysis.get('passed_tests', 0)}ä¸ª\n"
                    if analysis.get('failed_tests', 0) > 0:
                        md_content += f"- **å¤±è´¥è¯¦æƒ…**: {analysis.get('failed_tests', 0)}ä¸ªæµ‹è¯•å¤±è´¥\n"

            md_content += "\n"

        md_content += """## å»ºè®®å’Œç»“è®º

"""

        for recommendation in report['recommendations']:
            md_content += f"- {recommendation}\n"

        md_content += f"""

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        with open(output_file, "w", encoding="utf-8") as f:
            f.write(md_content)

    def run_comprehensive_tests(self) -> Dict:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        logging.info("å¼€å§‹PortMasterç»¼åˆæµ‹è¯•")
        logging.info("=" * 60)

        test_phases = [
            ("æ„å»ºéªŒè¯", self.run_build_verification),
            ("åŠŸèƒ½éªŒè¯", self.run_functionality_verification),
            ("å•å…ƒæµ‹è¯•", self.run_unit_tests),
            ("é›†æˆæµ‹è¯•", self.run_integration_tests),
            ("æ€§èƒ½æµ‹è¯•", self.run_performance_tests)
        ]

        for phase_name, phase_func in test_phases:
            try:
                phase_func()
            except Exception as e:
                logging.error(f"æµ‹è¯•é˜¶æ®µ {phase_name} å‘ç”Ÿå¼‚å¸¸: {e}")
                # ç»§ç»­æ‰§è¡Œåç»­æµ‹è¯•

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report()
        self.save_report(report)

        logging.info("=" * 60)
        logging.info("ç»¼åˆæµ‹è¯•å®Œæˆ")

        summary = report['test_summary']
        logging.info(f"æ€»é€šè¿‡ç‡: {summary['success_rate']:.1f}%")
        logging.info(f"é€šè¿‡é˜¶æ®µ: {summary['passed_phases']}/{summary['total_phases']}")

        if summary['success_rate'] >= 90:
            logging.info("ğŸ‰ æµ‹è¯•ç»“æœè‰¯å¥½ï¼")
        else:
            logging.warning("âš ï¸  å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œéœ€è¦æ£€æŸ¥å’Œä¿®å¤")

        return report

if __name__ == "__main__":
    runner = ComprehensiveTestRunner()
    report = runner.run_comprehensive_tests()

    # è®¾ç½®é€€å‡ºç 
    success_rate = report['test_summary']['success_rate']
    sys.exit(0 if success_rate >= 90 else 1)