#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–åˆ†æå™¨
è‡ªåŠ¨åˆ†ææ€§èƒ½ç“¶é¢ˆå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict
from datetime import datetime

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–åˆ†æå™¨"""

    def __init__(self):
        self.performance_db = Path("performance_data.json")
        self.optimization_history = Path("optimization_history.json")

    def analyze_performance(self, test_results: Dict) -> Dict:
        """
        åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ

        Args:
            test_results: æ€§èƒ½æµ‹è¯•ç»“æœ

        Returns:
            æ€§èƒ½åˆ†ææŠ¥å‘Š
        """
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "bottlenecks": [],
            "recommendations": [],
            "metrics": {},
            "severity": "low"
        }

        # åˆ†æååé‡
        throughput_analysis = self._analyze_throughput(test_results)
        if throughput_analysis:
            analysis["bottlenecks"].extend(throughput_analysis["bottlenecks"])
            analysis["recommendations"].extend(throughput_analysis["recommendations"])

        # åˆ†æå»¶è¿Ÿ
        latency_analysis = self._analyze_latency(test_results)
        if latency_analysis:
            analysis["bottlenecks"].extend(latency_analysis["bottlenecks"])
            analysis["recommendations"].extend(latency_analysis["recommendations"])

        # åˆ†æå†…å­˜ä½¿ç”¨
        memory_analysis = self._analyze_memory(test_results)
        if memory_analysis:
            analysis["bottlenecks"].extend(memory_analysis["bottlenecks"])
            analysis["recommendations"].extend(memory_analysis["recommendations"])

        # ç¡®å®šä¸¥é‡ç¨‹åº¦
        analysis["severity"] = self._determine_severity(analysis["bottlenecks"])

        # ä¿å­˜åˆ°å†å²æ•°æ®åº“
        self._save_analysis(analysis)

        return analysis

    def _analyze_throughput(self, test_results: Dict) -> Dict:
        """åˆ†æååé‡æ€§èƒ½"""
        analysis = {
            "bottlenecks": [],
            "recommendations": []
        }

        # æŸ¥æ‰¾ååé‡æµ‹è¯•ç»“æœ
        throughput_tests = [
            t for t in test_results.get('tests', [])
            if 'throughput' in t.get('name', '').lower()
        ]

        for test in throughput_tests:
            throughput = test.get('metrics', {}).get('throughput', 0)

            # ååé‡ä½äº1MB/sè§†ä¸ºç“¶é¢ˆ
            if throughput < 1024 * 1024:
                analysis["bottlenecks"].append({
                    "type": "throughput",
                    "test": test['name'],
                    "value": throughput,
                    "threshold": 1024 * 1024,
                    "description": f"ååé‡ä»…{throughput / 1024:.1f} KB/sï¼Œä½äº1 MB/sç›®æ ‡"
                })

                # æä¾›ä¼˜åŒ–å»ºè®®
                analysis["recommendations"].extend([
                    {
                        "priority": "high",
                        "category": "buffer_optimization",
                        "title": "å¢å¤§ç¼“å†²åŒºå¤§å°",
                        "description": "å½“å‰ååé‡è¾ƒä½ï¼Œå»ºè®®å¢å¤§å‘é€å’Œæ¥æ”¶ç¼“å†²åŒº",
                        "code_example": """
// åœ¨ReliableChannel.cppä¸­å¢å¤§ç¼“å†²åŒº
m_sendBuffer.resize(64 * 1024);  // å¢å¤§åˆ°64KB
m_recvBuffer.resize(64 * 1024);
                        """,
                        "expected_improvement": "æå‡30-50%ååé‡"
                    },
                    {
                        "priority": "medium",
                        "category": "window_size",
                        "title": "è°ƒæ•´æ»‘åŠ¨çª—å£å¤§å°",
                        "description": "å¢å¤§æ»‘åŠ¨çª—å£ä»¥å‡å°‘ç­‰å¾…ACKçš„æ—¶é—´",
                        "code_example": """
// åœ¨Protocolé…ç½®ä¸­è°ƒæ•´çª—å£å¤§å°
config.window_size = 16;  // å¢å¤§åˆ°16ï¼ˆå½“å‰ä¸º1ï¼‰
                        """,
                        "expected_improvement": "æå‡50-100%ååé‡"
                    }
                ])

        return analysis

    def _analyze_latency(self, test_results: Dict) -> Dict:
        """åˆ†æå»¶è¿Ÿæ€§èƒ½"""
        analysis = {
            "bottlenecks": [],
            "recommendations": []
        }

        # æŸ¥æ‰¾å»¶è¿Ÿæµ‹è¯•ç»“æœ
        latency_tests = [
            t for t in test_results.get('tests', [])
            if 'latency' in t.get('name', '').lower()
        ]

        for test in latency_tests:
            latency = test.get('metrics', {}).get('average_latency', 0)

            # å¹³å‡å»¶è¿Ÿè¶…è¿‡100msè§†ä¸ºç“¶é¢ˆ
            if latency > 100:
                analysis["bottlenecks"].append({
                    "type": "latency",
                    "test": test['name'],
                    "value": latency,
                    "threshold": 100,
                    "description": f"å¹³å‡å»¶è¿Ÿ{latency:.1f}msï¼Œè¶…è¿‡100msç›®æ ‡"
                })

                # æä¾›ä¼˜åŒ–å»ºè®®
                analysis["recommendations"].extend([
                    {
                        "priority": "high",
                        "category": "timeout_tuning",
                        "title": "ä¼˜åŒ–è¶…æ—¶å‚æ•°",
                        "description": "å‡å°é‡ä¼ è¶…æ—¶æ—¶é—´ä»¥åŠ å¿«é”™è¯¯æ¢å¤",
                        "code_example": """
// åœ¨ReliableChannelä¸­è°ƒæ•´è¶…æ—¶å‚æ•°
config.ack_timeout = 50;      // å‡å°åˆ°50msï¼ˆå½“å‰å¯èƒ½æ˜¯200msï¼‰
config.retry_interval = 100;  // è°ƒæ•´é‡è¯•é—´éš”
                        """,
                        "expected_improvement": "å‡å°‘20-30%å»¶è¿Ÿ"
                    },
                    {
                        "priority": "medium",
                        "category": "processing_optimization",
                        "title": "ä¼˜åŒ–å¸§å¤„ç†é€»è¾‘",
                        "description": "å‡å°‘ä¸å¿…è¦çš„æ•°æ®æ‹·è´å’Œå¤„ç†",
                        "code_example": """
// ä½¿ç”¨ç§»åŠ¨è¯­ä¹‰é¿å…æ•°æ®æ‹·è´
void OnFrameReceived(Frame&& frame) {
    m_receiveQueue.push(std::move(frame));  // ç§»åŠ¨è€Œéæ‹·è´
}
                        """,
                        "expected_improvement": "å‡å°‘10-15%å»¶è¿Ÿ"
                    }
                ])

        return analysis

    def _analyze_memory(self, test_results: Dict) -> Dict:
        """åˆ†æå†…å­˜ä½¿ç”¨"""
        analysis = {
            "bottlenecks": [],
            "recommendations": []
        }

        # æŸ¥æ‰¾å†…å­˜ç›¸å…³æµ‹è¯•ç»“æœ
        memory_tests = [
            t for t in test_results.get('tests', [])
            if 'memory' in t.get('name', '').lower() or 'stress' in t.get('name', '').lower()
        ]

        for test in memory_tests:
            peak_memory = test.get('metrics', {}).get('peak_memory_mb', 0)

            # å³°å€¼å†…å­˜è¶…è¿‡100MBè§†ä¸ºæ½œåœ¨é—®é¢˜
            if peak_memory > 100:
                analysis["bottlenecks"].append({
                    "type": "memory",
                    "test": test['name'],
                    "value": peak_memory,
                    "threshold": 100,
                    "description": f"å³°å€¼å†…å­˜ä½¿ç”¨{peak_memory:.1f}MBï¼Œè¶…è¿‡100MBå»ºè®®å€¼"
                })

                # æä¾›ä¼˜åŒ–å»ºè®®
                analysis["recommendations"].extend([
                    {
                        "priority": "medium",
                        "category": "memory_pool",
                        "title": "ä½¿ç”¨å†…å­˜æ± ç®¡ç†",
                        "description": "é¢‘ç¹åˆ†é…é‡Šæ”¾çš„å¯¹è±¡ä½¿ç”¨å†…å­˜æ± ",
                        "code_example": """
// åˆ›å»ºFrameå¯¹è±¡æ± 
class FramePool {
    std::vector<std::unique_ptr<Frame>> m_pool;
    std::mutex m_mutex;

public:
    std::unique_ptr<Frame> acquire() {
        std::lock_guard<std::mutex> lock(m_mutex);
        if (!m_pool.empty()) {
            auto frame = std::move(m_pool.back());
            m_pool.pop_back();
            return frame;
        }
        return std::make_unique<Frame>();
    }

    void release(std::unique_ptr<Frame> frame) {
        std::lock_guard<std::mutex> lock(m_mutex);
        frame->reset();
        m_pool.push_back(std::move(frame));
    }
};
                        """,
                        "expected_improvement": "å‡å°‘30-40%å†…å­˜åˆ†é…å¼€é”€"
                    },
                    {
                        "priority": "low",
                        "category": "buffer_reuse",
                        "title": "å¤ç”¨ç¼“å†²åŒº",
                        "description": "é¿å…æ¯æ¬¡å‘é€éƒ½åˆ†é…æ–°ç¼“å†²åŒº",
                        "code_example": """
// åœ¨Transportå±‚å¤ç”¨ç¼“å†²åŒº
class Transport {
    std::vector<uint8_t> m_sendBuffer;  // æˆå‘˜å˜é‡ï¼Œå¤ç”¨

    void Send(const void* data, size_t size) {
        m_sendBuffer.resize(size);  // è°ƒæ•´å¤§å°è€Œéé‡æ–°åˆ†é…
        memcpy(m_sendBuffer.data(), data, size);
        // ... å‘é€é€»è¾‘
    }
};
                        """,
                        "expected_improvement": "å‡å°‘20-30%å†…å­˜åˆ†é…æ¬¡æ•°"
                    }
                ])

        return analysis

    def _determine_severity(self, bottlenecks: List[Dict]) -> str:
        """ç¡®å®šæ€§èƒ½é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
        if not bottlenecks:
            return "none"

        # æ ¹æ®ç“¶é¢ˆæ•°é‡å’Œç±»å‹åˆ¤æ–­
        high_priority_count = sum(1 for b in bottlenecks if b['type'] in ['throughput', 'latency'])

        if high_priority_count >= 3:
            return "critical"
        elif high_priority_count >= 2:
            return "high"
        elif high_priority_count >= 1:
            return "medium"
        else:
            return "low"

    def _save_analysis(self, analysis: Dict):
        """ä¿å­˜åˆ†æç»“æœåˆ°å†å²æ•°æ®åº“"""
        history = []
        if self.optimization_history.exists():
            with open(self.optimization_history, 'r', encoding='utf-8') as f:
                history = json.load(f)

        history.append(analysis)

        # ä¿ç•™æœ€è¿‘30æ¬¡åˆ†æ
        if len(history) > 30:
            history = history[-30:]

        with open(self.optimization_history, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)

    def generate_optimization_report(self, analysis: Dict) -> str:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®æŠ¥å‘Š

        Args:
            analysis: æ€§èƒ½åˆ†æç»“æœ

        Returns:
            Markdownæ ¼å¼æŠ¥å‘Š
        """
        report = f"""# æ€§èƒ½ä¼˜åŒ–å»ºè®®æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ä¸¥é‡ç¨‹åº¦**: {self._severity_emoji(analysis['severity'])} {analysis['severity'].upper()}

---

## ğŸ“Š æ€§èƒ½ç“¶é¢ˆ

"""

        if not analysis["bottlenecks"]:
            report += "âœ… æœªå‘ç°æ˜æ˜¾æ€§èƒ½ç“¶é¢ˆ\n\n"
        else:
            for idx, bottleneck in enumerate(analysis["bottlenecks"], 1):
                report += f"""### {idx}. {bottleneck['type'].upper()} - {bottleneck['test']}

- **å½“å‰å€¼**: {self._format_metric(bottleneck['value'], bottleneck['type'])}
- **ç›®æ ‡å€¼**: {self._format_metric(bottleneck['threshold'], bottleneck['type'])}
- **æè¿°**: {bottleneck['description']}

"""

        report += """---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

"""

        if not analysis["recommendations"]:
            report += "æš‚æ— ä¼˜åŒ–å»ºè®®\n\n"
        else:
            # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
            by_priority = defaultdict(list)
            for rec in analysis["recommendations"]:
                by_priority[rec['priority']].append(rec)

            for priority in ['high', 'medium', 'low']:
                recs = by_priority.get(priority, [])
                if not recs:
                    continue

                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
                report += f"### {priority_emoji[priority]} {priority.upper()}ä¼˜å…ˆçº§\n\n"

                for idx, rec in enumerate(recs, 1):
                    report += f"""#### {idx}. {rec['title']}

**ç±»åˆ«**: {rec['category']}

**æè¿°**: {rec['description']}

**ä»£ç ç¤ºä¾‹**:
```cpp
{rec['code_example'].strip()}
```

**é¢„æœŸæ•ˆæœ**: {rec['expected_improvement']}

---

"""

        report += f"""## ğŸ“ˆ å†å²è¶‹åŠ¿

æŸ¥çœ‹å®Œæ•´å†å²æ•°æ®: `{self.optimization_history}`

"""

        return report

    def _severity_emoji(self, severity: str) -> str:
        """è·å–ä¸¥é‡ç¨‹åº¦emoji"""
        return {
            'none': 'âœ…',
            'low': 'ğŸŸ¢',
            'medium': 'ğŸŸ¡',
            'high': 'ğŸŸ ',
            'critical': 'ğŸ”´'
        }.get(severity, 'â“')

    def _format_metric(self, value: float, metric_type: str) -> str:
        """æ ¼å¼åŒ–æ€§èƒ½æŒ‡æ ‡"""
        if metric_type == 'throughput':
            if value >= 1024 * 1024:
                return f"{value / (1024 * 1024):.2f} MB/s"
            elif value >= 1024:
                return f"{value / 1024:.2f} KB/s"
            else:
                return f"{value:.2f} B/s"
        elif metric_type == 'latency':
            return f"{value:.2f} ms"
        elif metric_type == 'memory':
            return f"{value:.2f} MB"
        else:
            return f"{value:.2f}"

    def compare_with_baseline(self, current_results: Dict, baseline_version: str) -> Dict:
        """
        ä¸åŸºçº¿ç‰ˆæœ¬å¯¹æ¯”æ€§èƒ½

        Args:
            current_results: å½“å‰æµ‹è¯•ç»“æœ
            baseline_version: åŸºçº¿ç‰ˆæœ¬å·

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        baseline_path = Path(f"AutoTest/baselines/baseline_{baseline_version}.json")
        if not baseline_path.exists():
            return {"error": f"åŸºçº¿ç‰ˆæœ¬ {baseline_version} ä¸å­˜åœ¨"}

        with open(baseline_path, 'r', encoding='utf-8') as f:
            baseline = json.load(f)

        comparison = {
            "baseline_version": baseline_version,
            "improvements": [],
            "regressions": [],
            "unchanged": []
        }

        # å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡
        for current_test in current_results.get('tests', []):
            test_name = current_test['name']
            baseline_test = next((t for t in baseline.get('tests', []) if t['name'] == test_name), None)

            if not baseline_test:
                continue

            current_metrics = current_test.get('metrics', {})
            baseline_metrics = baseline_test.get('metrics', {})

            # å¯¹æ¯”ååé‡
            if 'throughput' in current_metrics and 'throughput' in baseline_metrics:
                current_val = current_metrics['throughput']
                baseline_val = baseline_metrics['throughput']
                change = (current_val - baseline_val) / baseline_val * 100

                if abs(change) < 5:  # å˜åŒ–å°äº5%è§†ä¸ºæ— å˜åŒ–
                    comparison["unchanged"].append({
                        "test": test_name,
                        "metric": "throughput",
                        "change": change
                    })
                elif change > 0:
                    comparison["improvements"].append({
                        "test": test_name,
                        "metric": "throughput",
                        "current": current_val,
                        "baseline": baseline_val,
                        "improvement": change
                    })
                else:
                    comparison["regressions"].append({
                        "test": test_name,
                        "metric": "throughput",
                        "current": current_val,
                        "baseline": baseline_val,
                        "regression": abs(change)
                    })

        return comparison

def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    optimizer = PerformanceOptimizer()

    # ç¤ºä¾‹æµ‹è¯•ç»“æœ
    test_results = {
        "tests": [
            {
                "name": "ThroughputTest",
                "metrics": {
                    "throughput": 500 * 1024  # 500 KB/s (ä½äº1MB/sç›®æ ‡)
                }
            },
            {
                "name": "LatencyTest",
                "metrics": {
                    "average_latency": 150  # 150ms (è¶…è¿‡100msç›®æ ‡)
                }
            },
            {
                "name": "StressTest",
                "metrics": {
                    "peak_memory_mb": 120  # 120MB (è¶…è¿‡100MBå»ºè®®å€¼)
                }
            }
        ]
    }

    # åˆ†ææ€§èƒ½
    analysis = optimizer.analyze_performance(test_results)

    # ç”ŸæˆæŠ¥å‘Š
    report = optimizer.generate_optimization_report(analysis)
    print(report)

    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("performance_optimization_report.md")
    report_file.write_text(report, encoding='utf-8')
    print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    main()
