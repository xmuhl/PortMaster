# AI Development System - ä½¿ç”¨ç¤ºä¾‹

## ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [åŸºæœ¬ç”¨æ³•ç¤ºä¾‹](#åŸºæœ¬ç”¨æ³•ç¤ºä¾‹)
3. [é«˜çº§ç”¨æ³•ç¤ºä¾‹](#é«˜çº§ç”¨æ³•ç¤ºä¾‹)
4. [é›†æˆç¤ºä¾‹](#é›†æˆç¤ºä¾‹)
5. [é…ç½®ç¤ºä¾‹](#é…ç½®ç¤ºä¾‹)
6. [å·¥ä½œæµç¤ºä¾‹](#å·¥ä½œæµç¤ºä¾‹)
7. [æ’ä»¶å¼€å‘ç¤ºä¾‹](#æ’ä»¶å¼€å‘ç¤ºä¾‹)

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ç³»ç»Ÿ

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-repo/PortMaster-AI-Dev-System.git
cd PortMaster-AI-Dev-System

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python main.py --help
```

### 2. ç¬¬ä¸€æ¬¡åˆ†æ

```bash
# åˆ†ææ‚¨çš„é¡¹ç›®
python main.py analyze /path/to/your/project

# æŸ¥çœ‹HTMLæŠ¥å‘Š
open output/reports/index.html
```

### 3. ç¼–ç¨‹æ¥å£

```python
from ai_dev_system import MasterController, ConfigManager
from pathlib import Path

# åˆ›å»ºé…ç½®
config = ConfigManager()

# è¿è¡Œåˆ†æ
controller = MasterController(Path("/path/to/project"), config)
results = controller.run_full_workflow()

print(f"æ„å»ºçŠ¶æ€: {'æˆåŠŸ' if results['build']['success'] else 'å¤±è´¥'}")
```

## åŸºæœ¬ç”¨æ³•ç¤ºä¾‹

### 1. é¡¹ç›®åˆ†æ

#### å®Œæ•´åˆ†æ

```bash
# åˆ†æé¡¹ç›®ï¼ˆåŒ…æ‹¬æ„å»ºã€æµ‹è¯•ã€è¦†ç›–ç‡ç­‰ï¼‰
python main.py analyze ./my-python-project

# åªåˆ†æä»£ç è´¨é‡ï¼Œè·³è¿‡æ„å»º
python main.py analyze ./my-project --skip-build

# è·³è¿‡æµ‹è¯•æ‰§è¡Œ
python main.py analyze ./my-project --skip-tests
```

#### å•ç‹¬åˆ†æ

```bash
# åªæ„å»ºé¡¹ç›®
python main.py build ./my-project --type debug

# åªè¿è¡Œæµ‹è¯•
python main.py test ./my-project --coverage

# ç”ŸæˆæŠ¥å‘Š
python main.py report ./my-project --format html
```

### 2. è‡ªåŠ¨ä¿®å¤

```python
from ai_dev_system import MasterController, FixController, ConfigManager
from pathlib import Path

# é…ç½®
config = ConfigManager()
project_path = Path("./my-project")

# åˆ†æé¡¹ç›®
controller = MasterController(project_path, config)
analysis_results = controller.run_full_workflow()

# æ”¶é›†æ‰€æœ‰é—®é¢˜
issues = []
issues.extend(analysis_results.get('static_analysis', {}).get('issues', []))
issues.extend(analysis_results.get('performance', {}).get('issues', []))

print(f"å‘ç° {len(issues)} ä¸ªé—®é¢˜")

# é¢„è§ˆä¿®å¤ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰
fix_controller = FixController(project_path, config)
preview = fix_controller.preview_fixes(issues)

for fix in preview[:5]:  # æ˜¾ç¤ºå‰5ä¸ªä¿®å¤é¢„è§ˆ
    print(f"æ–‡ä»¶: {fix['file']}")
    print(f"é—®é¢˜: {fix['issue']}")
    print(f"å»ºè®®ä¿®å¤: {fix['suggested_fix']}")
    print("---")

# åº”ç”¨è‡ªåŠ¨ä¿®å¤
if input("æ˜¯å¦åº”ç”¨è‡ªåŠ¨ä¿®å¤? (y/n): ").lower() == 'y':
    fix_results = fix_controller.apply_fixes(issues, backup=True)
    print(f"æˆåŠŸä¿®å¤ {fix_results['successful_fixes']} ä¸ªé—®é¢˜")
    print(f"å¤±è´¥ {fix_results['failed_fixes']} ä¸ªé—®é¢˜")
```

### 3. æµ‹è¯•å’Œè¦†ç›–ç‡

```python
from ai_dev_system import TestIntegrator, ConfigManager
from pathlib import Path

config = ConfigManager()
project_path = Path("./my-project")

# åˆ›å»ºæµ‹è¯•é›†æˆå™¨
test_integrator = TestIntegrator(project_path, config)

# è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†è¦†ç›–ç‡
result = test_integrator.execute_tests(include_coverage=True)

print(f"æµ‹è¯•ç»“æœ:")
print(f"  æ€»æµ‹è¯•æ•°: {result.total_tests}")
print(f"  é€šè¿‡: {result.passed_tests}")
print(f"  å¤±è´¥: {result.failed_tests}")
print(f"  è·³è¿‡: {result.skipped_tests}")
print(f"  è€—æ—¶: {result.duration:.2f}s")

if result.coverage_info:
    coverage = result.coverage_info
    print(f"\nè¦†ç›–ç‡ä¿¡æ¯:")
    print(f"  è¡Œè¦†ç›–ç‡: {coverage['line_coverage']:.1f}%")
    print(f"  åˆ†æ”¯è¦†ç›–ç‡: {coverage['branch_coverage']:.1f}%")
    print(f"  å‡½æ•°è¦†ç›–ç‡: {coverage['function_coverage']:.1f}%")

    # æ‰¾å‡ºè¦†ç›–ç‡ä½çš„æ–‡ä»¶
    low_coverage_files = []
    for file_path, file_coverage in coverage.get('files', {}).items():
        if file_coverage.get('line_coverage', 0) < 50:
            low_coverage_files.append((file_path, file_coverage['line_coverage']))

    if low_coverage_files:
        print(f"\nè¦†ç›–ç‡è¾ƒä½çš„æ–‡ä»¶:")
        for file_path, cov in sorted(low_coverage_files, key=lambda x: x[1]):
            print(f"  {file_path}: {cov:.1f}%")
```

## é«˜çº§ç”¨æ³•ç¤ºä¾‹

### 1. è‡ªå®šä¹‰åˆ†æè§„åˆ™

```python
from ai_dev_system import StaticAnalyzer, ConfigManager
from pathlib import Path

config = ConfigManager()
project_path = Path("./my-project")

# åˆ›å»ºé™æ€åˆ†æå™¨
analyzer = StaticAnalyzer(project_path, config)

# æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
custom_rules = [
    {
        'id': 'custom_naming_convention',
        'name': 'å‡½æ•°å‘½åè§„èŒƒ',
        'pattern': r'def\s+([a-z]+_[a-z_]+)',
        'message': 'å‡½æ•°ååº”ä½¿ç”¨é©¼å³°å‘½åæ³•',
        'severity': 'medium',
        'suggestion': 'å»ºè®®ä½¿ç”¨é©¼å³°å‘½åæ³•ï¼Œå¦‚ï¼šgetUserName',
        'language': 'python'
    },
    {
        'id': 'custom_docstring_required',
        'name': 'ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²',
        'pattern': r'def\s+\w+\([^)]*\):(?!\s*"""|\s*\'\'\')',
        'message': 'å…¬å…±å‡½æ•°ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²',
        'severity': 'low',
        'suggestion': 'ä¸ºå‡½æ•°æ·»åŠ è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²',
        'language': 'python'
    }
]

# åº”ç”¨è‡ªå®šä¹‰è§„åˆ™
for rule in custom_rules:
    analyzer.add_custom_rule(rule)

# è¿è¡Œåˆ†æ
result = analyzer.analyze()

# æŸ¥çœ‹è‡ªå®šä¹‰è§„åˆ™çš„ç»“æœ
custom_issues = [issue for issue in result.issues
                if issue.get('rule_id') in ['custom_naming_convention', 'custom_docstring_required']]

print(f"è‡ªå®šä¹‰è§„åˆ™å‘ç° {len(custom_issues)} ä¸ªé—®é¢˜:")
for issue in custom_issues:
    print(f"  {issue['file_path']}:{issue['line_number']} - {issue['message']}")
```

### 2. æ‰¹é‡é¡¹ç›®åˆ†æ

```python
from ai_dev_system import analyze_project, ConfigManager
from pathlib import Path
import concurrent.futures
import json
from datetime import datetime

def analyze_single_project(project_path, output_dir):
    """åˆ†æå•ä¸ªé¡¹ç›®"""
    try:
        print(f"å¼€å§‹åˆ†æé¡¹ç›®: {project_path.name}")

        # åˆ›å»ºé¡¹ç›®ç‰¹å®šçš„è¾“å‡ºç›®å½•
        project_output_dir = output_dir / project_path.name
        project_output_dir.mkdir(exist_ok=True)

        # é…ç½®åˆ†æ
        config = ConfigManager()
        config.set('reporting.output_directory', str(project_output_dir / 'reports'))

        # è¿è¡Œåˆ†æ
        results = analyze_project(project_path, config)

        # ä¿å­˜ç»“æœ
        result_file = project_output_dir / 'analysis_results.json'
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        return {
            'project': str(project_path),
            'status': 'success',
            'results': results,
            'output_file': str(result_file)
        }

    except Exception as e:
        return {
            'project': str(project_path),
            'status': 'error',
            'error': str(e)
        }

# æ‰¹é‡åˆ†æé…ç½®
projects_dir = Path("/path/to/projects")
output_dir = Path("/path/to/batch-analysis-results")
output_dir.mkdir(exist_ok=True)

# å‘ç°æ‰€æœ‰é¡¹ç›®
projects = [p for p in projects_dir.iterdir() if p.is_dir()]
print(f"å‘ç° {len(projects)} ä¸ªé¡¹ç›®")

# å¹¶è¡Œåˆ†æï¼ˆæœ€å¤š4ä¸ªå¹¶å‘ï¼‰
max_workers = 4
all_results = []

with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
    # æäº¤æ‰€æœ‰åˆ†æä»»åŠ¡
    future_to_project = {
        executor.submit(analyze_single_project, project, output_dir): project
        for project in projects
    }

    # æ”¶é›†ç»“æœ
    for future in concurrent.futures.as_completed(future_to_project):
        project = future_to_project[future]
        try:
            result = future.result()
            all_results.append(result)

            if result['status'] == 'success':
                print(f"âœ… {project.name} åˆ†æå®Œæˆ")
            else:
                print(f"âŒ {project.name} åˆ†æå¤±è´¥: {result['error']}")

        except Exception as e:
            print(f"âŒ {project.name} åˆ†æå¼‚å¸¸: {e}")
            all_results.append({
                'project': str(project),
                'status': 'error',
                'error': str(e)
            })

# ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
summary = {
    'timestamp': datetime.now().isoformat(),
    'total_projects': len(projects),
    'successful_analyses': len([r for r in all_results if r['status'] == 'success']),
    'failed_analyses': len([r for r in all_results if r['status'] == 'error']),
    'results': all_results
}

summary_file = output_dir / 'batch_analysis_summary.json'
with open(summary_file, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False, default=str)

print(f"\nåˆ†æå®Œæˆ!")
print(f"æˆåŠŸ: {summary['successful_analyses']}")
print(f"å¤±è´¥: {summary['failed_analyses']}")
print(f"æ±‡æ€»æŠ¥å‘Š: {summary_file}")
```

### 3. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
from ai_dev_system import PerformanceAnalyzer, MasterController, ConfigManager
from pathlib import Path
import time
import statistics

class PerformanceBenchmark:
    def __init__(self, project_path):
        self.project_path = project_path
        self.config = ConfigManager()
        self.analyzer = PerformanceAnalyzer(project_path, self.config)
        self.controller = MasterController(project_path, self.config)

    def benchmark_full_analysis(self, iterations=3):
        """åŸºå‡†æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹"""
        durations = []
        memory_usage = []

        print(f"å¼€å§‹å®Œæ•´åˆ†æåŸºå‡†æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)...")

        for i in range(iterations):
            print(f"ç¬¬ {i+1} æ¬¡è¿­ä»£...")

            start_time = time.time()
            start_memory = self._get_memory_usage()

            # è¿è¡Œå®Œæ•´åˆ†æ
            results = self.controller.run_full_workflow()

            end_time = time.time()
            end_memory = self._get_memory_usage()

            duration = end_time - start_time
            memory_delta = end_memory - start_memory

            durations.append(duration)
            memory_usage.append(memory_delta)

            print(f"  è€—æ—¶: {duration:.2f}s, å†…å­˜å¢é‡: {memory_delta:.1f}MB")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_duration = statistics.mean(durations)
        std_duration = statistics.stdev(durations) if len(durations) > 1 else 0
        avg_memory = statistics.mean(memory_usage)

        return {
            'iterations': iterations,
            'avg_duration': avg_duration,
            'std_duration': std_duration,
            'min_duration': min(durations),
            'max_duration': max(durations),
            'avg_memory_usage': avg_memory,
            'durations': durations,
            'memory_usage': memory_usage
        }

    def benchmark_individual_components(self):
        """åŸºå‡†æµ‹è¯•å„ä¸ªç»„ä»¶"""
        results = {}

        # æµ‹è¯•é™æ€åˆ†æ
        print("åŸºå‡†æµ‹è¯•é™æ€åˆ†æ...")
        start_time = time.time()
        static_result = self.analyzer.analyze_performance()
        static_duration = time.time() - start_time
        results['static_analysis'] = {
            'duration': static_duration,
            'issues_found': len(static_result)
        }

        # æµ‹è¯•æ„å»ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('build.enabled', True):
            print("åŸºå‡†æµ‹è¯•æ„å»º...")
            from ai_dev_system import BuildIntegrator
            build_integrator = BuildIntegrator(self.project_path, self.config)
            start_time = time.time()
            build_result = build_integrator.execute_build()
            build_duration = time.time() - start_time
            results['build'] = {
                'duration': build_duration,
                'success': build_result.success,
                'warnings': build_result.warnings_count,
                'errors': build_result.errors_count
            }

        # æµ‹è¯•æµ‹è¯•æ‰§è¡Œï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('test.enabled', True):
            print("åŸºå‡†æµ‹è¯•æµ‹è¯•æ‰§è¡Œ...")
            from ai_dev_system import TestIntegrator
            test_integrator = TestIntegrator(self.project_path, self.config)
            start_time = time.time()
            test_result = test_integrator.execute_tests()
            test_duration = time.time() - start_time
            results['test'] = {
                'duration': test_duration,
                'total_tests': test_result.total_tests,
                'passed_tests': test_result.passed_tests,
                'failed_tests': test_result.failed_tests
            }

        return results

    def _get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    project_path = Path("./my-project")
    benchmark = PerformanceBenchmark(project_path)

    # å®Œæ•´åˆ†æåŸºå‡†æµ‹è¯•
    full_analysis_results = benchmark.benchmark_full_analysis(iterations=5)
    print("\nå®Œæ•´åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ:")
    print(f"  å¹³å‡è€—æ—¶: {full_analysis_results['avg_duration']:.2f}s Â± {full_analysis_results['std_duration']:.2f}s")
    print(f"  æœ€çŸ­è€—æ—¶: {full_analysis_results['min_duration']:.2f}s")
    print(f"  æœ€é•¿è€—æ—¶: {full_analysis_results['max_duration']:.2f}s")
    print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {full_analysis_results['avg_memory_usage']:.1f}MB")

    # ç»„ä»¶åŸºå‡†æµ‹è¯•
    component_results = benchmark.benchmark_individual_components()
    print("\nç»„ä»¶åŸºå‡†æµ‹è¯•ç»“æœ:")
    for component, result in component_results.items():
        print(f"  {component}: {result['duration']:.2f}s")
        for key, value in result.items():
            if key != 'duration':
                print(f"    {key}: {value}")
```

## é›†æˆç¤ºä¾‹

### 1. Flask Webåº”ç”¨é›†æˆ

```python
from flask import Flask, request, jsonify, render_template
from ai_dev_system import MasterController, ConfigManager, ReportGenerator
from pathlib import Path
import tempfile
import shutil
import json

app = Flask(__name__)

# é…ç½®
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
ALLOWED_EXTENSIONS = {'zip', 'tar', 'gz'}

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['OUTPUT_FOLDER'] = OUTPUT_FOLDER

# ç¡®ä¿ç›®å½•å­˜åœ¨
Path(UPLOAD_FOLDER).mkdir(exist_ok=True)
Path(OUTPUT_FOLDER).mkdir(exist_ok=True)

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/analyze', methods=['POST'])
def analyze_project():
    """åˆ†æä¸Šä¼ çš„é¡¹ç›®"""
    try:
        # æ£€æŸ¥æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400

        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

        if not allowed_file(file.filename):
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_path = temp_path / file.filename
            file.save(file_path)

            # è§£å‹é¡¹ç›®
            project_dir = temp_path / 'project'
            if file.filename.endswith('.zip'):
                import zipfile
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(project_dir)
            else:
                import tarfile
                with tarfile.open(file_path, 'r:*') as tar_ref:
                    tar_ref.extractall(project_dir)

            # è¿è¡Œåˆ†æ
            config = ConfigManager()
            config.set('reporting.output_formats', ['json'])

            controller = MasterController(project_dir, config)
            results = controller.run_full_workflow()

            # ç”ŸæˆæŠ¥å‘Š
            output_dir = Path(app.config['OUTPUT_FOLDER']) / f"analysis_{int(time.time())}"
            output_dir.mkdir(exist_ok=True)

            generator = ReportGenerator(output_dir, config)
            html_report = generator.generate_report(results, format='html')

            # ä¿å­˜JSONç»“æœ
            json_file = output_dir / 'results.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            # è¿”å›ç»“æœæ‘˜è¦
            return jsonify({
                'status': 'success',
                'project_name': file.filename,
                'analysis_id': output_dir.name,
                'summary': {
                    'build_success': results['build']['success'],
                    'total_tests': results['test']['total_tests'],
                    'passed_tests': results['test']['passed_tests'],
                    'coverage': results.get('coverage', {}).get('line_coverage', 0),
                    'total_issues': len(results.get('static_analysis', {}).get('issues', [])),
                    'performance_issues': len(results.get('performance', {}).get('issues', []))
                },
                'report_url': f"/report/{output_dir.name}",
                'download_url': f"/download/{output_dir.name}/results.json"
            })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/report/<analysis_id>')
def view_report(analysis_id):
    """æŸ¥çœ‹åˆ†ææŠ¥å‘Š"""
    report_path = Path(app.config['OUTPUT_FOLDER']) / analysis_id / 'index.html'
    if report_path.exists():
        return send_file(report_path)
    else:
        return "æŠ¥å‘Šä¸å­˜åœ¨", 404

@app.route('/download/<analysis_id>/<filename>')
def download_file(analysis_id, filename):
    """ä¸‹è½½åˆ†ææ–‡ä»¶"""
    file_path = Path(app.config['OUTPUT_FOLDER']) / analysis_id / filename
    if file_path.exists():
        return send_file(file_path, as_attachment=True)
    else:
        return "æ–‡ä»¶ä¸å­˜åœ¨", 404

@app.route('/api/health')
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'timestamp': time.time(),
        'version': '1.0.0'
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
```

### 2. ä¸Gité›†æˆ

```python
from ai_dev_system import MasterController, ConfigManager, GitManager
from pathlib import Path
import argparse
import sys

def analyze_git_diff(repo_path, commit_hash=None):
    """åˆ†æGitå·®å¼‚"""
    try:
        # åˆå§‹åŒ–Gitç®¡ç†å™¨
        git_manager = GitManager(repo_path)

        # è·å–å·®å¼‚
        diff_content = git_manager.get_diff(commit_hash)
        if not diff_content:
            print("æ²¡æœ‰å‘ç°å·®å¼‚")
            return

        print(f"åˆ†æGitå·®å¼‚...")
        if commit_hash:
            print(f"ä¸æäº¤ {commit_hash} çš„å·®å¼‚")
        else:
            print("å·¥ä½œç›®å½•çš„å·®å¼‚")

        # æå–ä¿®æ”¹çš„æ–‡ä»¶
        status = git_manager.get_status()
        modified_files = status['modified_files'] + status['staged_files']

        if not modified_files:
            print("æ²¡æœ‰ä¿®æ”¹çš„æ–‡ä»¶")
            return

        print(f"å‘ç° {len(modified_files)} ä¸ªä¿®æ”¹çš„æ–‡ä»¶:")
        for file_path in modified_files:
            print(f"  - {file_path}")

        # è¿è¡Œåˆ†æ
        config = ConfigManager()
        config.set('static_analysis.include_files', modified_files)

        controller = MasterController(repo_path, config)

        # åªè¿è¡Œé™æ€åˆ†æ
        results = controller.run_static_analysis()

        # æ˜¾ç¤ºç»“æœ
        issues = results.get('issues', [])
        if issues:
            print(f"\nå‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues:
                if issue['file_path'] in modified_files:
                    print(f"  {issue['file_path']}:{issue.get('line_number', '?')} - {issue['message']}")
                    print(f"    ä¸¥é‡ç¨‹åº¦: {issue['severity']}")
                    if issue.get('suggestion'):
                        print(f"    å»ºè®®: {issue['suggestion']}")
                    print()
        else:
            print("æ²¡æœ‰å‘ç°é—®é¢˜")

        # å¯é€‰ï¼šè‡ªåŠ¨ä¿®å¤
        if issues and input("æ˜¯å¦å°è¯•è‡ªåŠ¨ä¿®å¤? (y/n): ").lower() == 'y':
            from ai_dev_system import FixController
            fix_controller = FixController(repo_path, config)
            fix_results = fix_controller.apply_fixes(issues, backup=True)
            print(f"ä¿®å¤äº† {fix_results['successful_fixes']} ä¸ªé—®é¢˜")

    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description='åˆ†æGitå·®å¼‚')
    parser.add_argument('repo_path', type=Path, help='Gitä»“åº“è·¯å¾„')
    parser.add_argument('--commit', help='æŒ‡å®šæäº¤å“ˆå¸Œ')

    args = parser.parse_args()

    if not args.repo_path.exists():
        print(f"è·¯å¾„ä¸å­˜åœ¨: {args.repo_path}")
        sys.exit(1)

    analyze_git_diff(args.repo_path, args.commit)

if __name__ == '__main__':
    main()
```

### 3. CI/CDé›†æˆè„šæœ¬

```bash
#!/bin/bash
# ci-integration.sh - CI/CDé›†æˆè„šæœ¬

set -e

# é…ç½®
PROJECT_PATH="${1:-.}"
OUTPUT_DIR="${2:-./ci-output}"
CONFIG_FILE="${3:-./ci-config.yaml}"

echo "å¼€å§‹CI/CDåˆ†æ..."
echo "é¡¹ç›®è·¯å¾„: $PROJECT_PATH"
echo "è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$OUTPUT_DIR"

# æ£€æŸ¥Pythonç¯å¢ƒ
if ! command -v python &> /dev/null; then
    echo "é”™è¯¯: Pythonæœªå®‰è£…"
    exit 1
fi

# å®‰è£…ç³»ç»Ÿï¼ˆå¦‚æœéœ€è¦ï¼‰
if [ ! -d "PortMaster-AI-Dev-System" ]; then
    echo "å…‹éš†AIå¼€å‘ç³»ç»Ÿ..."
    git clone https://github.com/your-repo/PortMaster-AI-Dev-System.git
    cd PortMaster-AI-Dev-System
    pip install -r requirements.txt
    cd ..
else
    cd PortMaster-AI-Dev-System
    git pull
    pip install -r requirements.txt
    cd ..
fi

# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH="${PYTHONPATH}:$(pwd)/PortMaster-AI-Dev-System"

# åˆ›å»ºCIé…ç½®
cat > "$CONFIG_FILE" << EOF
system:
  log_level: "INFO"
  output_directory: "$OUTPUT_DIR"

build:
  enabled: true
  timeout: 300

test:
  enabled: true
  timeout: 600
  coverage_threshold: 80.0
  collect_coverage: true

static_analysis:
  enabled: true
  complexity_threshold: 10
  security_scan: true

reporting:
  output_formats:
    - "json"
    - "markdown"
  include_charts: true

performance:
  enable_profiling: false

logging:
  level: "INFO"
  file: "$OUTPUT_DIR/ci-analysis.log"
EOF

echo "è¿è¡Œåˆ†æ..."

# è¿è¡Œåˆ†æ
cd PortMaster-AI-Dev-System
python main.py analyze "$PROJECT_PATH" --config "$CONFIG_FILE" --output "$OUTPUT_DIR"
cd ..

# æ£€æŸ¥ç»“æœ
if [ -f "$OUTPUT_DIR/reports/analysis_results.json" ]; then
    echo "åˆ†æå®Œæˆï¼Œç”ŸæˆæŠ¥å‘Š..."

    # æå–å…³é”®æŒ‡æ ‡
    python3 << EOF
import json
import sys

with open("$OUTPUT_DIR/reports/analysis_results.json", 'r') as f:
    results = json.load(f)

# æ„å»ºçŠ¶æ€
build_success = results.get('build', {}).get('success', False)
print(f"æ„å»ºçŠ¶æ€: {'æˆåŠŸ' if build_success else 'å¤±è´¥'}")

if not build_success:
    print("æ„å»ºå¤±è´¥ï¼ŒCIæ£€æŸ¥æœªé€šè¿‡")
    sys.exit(1)

# æµ‹è¯•ç»“æœ
test_results = results.get('test', {})
total_tests = test_results.get('total_tests', 0)
passed_tests = test_results.get('passed_tests', 0)
failed_tests = test_results.get('failed_tests', 0)

print(f"æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")

if failed_tests > 0:
    print(f"å­˜åœ¨ {failed_tests} ä¸ªå¤±è´¥çš„æµ‹è¯•")
    sys.exit(1)

# è¦†ç›–ç‡
coverage = results.get('coverage', {}).get('line_coverage', 0)
print(f"ä»£ç è¦†ç›–ç‡: {coverage:.1f}%")

if coverage < 80.0:
    print(f"è¦†ç›–ç‡ {coverage:.1f}% ä½äºè¦æ±‚çš„ 80%")
    sys.exit(1)

# ä»£ç è´¨é‡
issues = results.get('static_analysis', {}).get('issues', [])
critical_issues = [i for i in issues if i.get('severity') == 'critical']
high_issues = [i for i in issues if i.get('severity') == 'high']

print(f"ä»£ç è´¨é‡: å‘ç° {len(issues)} ä¸ªé—®é¢˜")
print(f"  ä¸¥é‡é—®é¢˜: {len(critical_issues)}")
print(f"  é«˜ä¼˜å…ˆçº§é—®é¢˜: {len(high_issues)}")

if critical_issues:
    print("å­˜åœ¨ä¸¥é‡ä»£ç è´¨é‡é—®é¢˜")
    sys.exit(1)

print("æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
EOF

    echo "CI/CDåˆ†æå®Œæˆï¼"
    echo "è¯¦ç»†æŠ¥å‘Š: $OUTPUT_DIR/reports/"

    # å¦‚æœåœ¨CIç¯å¢ƒä¸­ï¼Œä¸Šä¼ æŠ¥å‘Š
    if [ -n "$CI" ]; then
        echo "ä¸Šä¼ æŠ¥å‘Šåˆ°CIç³»ç»Ÿ..."

        # GitHub Actions
        if [ "$CI_SYSTEM" = "github-actions" ]; then
            echo "::set-output name=report-path::$OUTPUT_DIR/reports/"

            # åˆ›å»ºPRè¯„è®ºï¼ˆå¦‚æœåœ¨PRä¸­ï¼‰
            if [ -n "$GITHUB_EVENT_NAME" ] && [ "$GITHUB_EVENT_NAME" = "pull_request" ]; then
                python3 << EOF
import json
import os
import requests

with open("$OUTPUT_DIR/reports/analysis_results.json", 'r') as f:
    results = json.load(f)

# æ„å»ºè¯„è®ºå†…å®¹
comment = f"""
## ğŸ¤– AIå¼€å‘ç³»ç»Ÿåˆ†æç»“æœ

### âœ… é€šè¿‡é¡¹ç›®
- âœ… æ„å»ºçŠ¶æ€: {'æˆåŠŸ' if results.get('build', {}).get('success', False) else 'å¤±è´¥'}
- âœ… æµ‹è¯•é€šè¿‡ç‡: {results.get('test', {}).get('passed_tests', 0)}/{results.get('test', {}).get('total_tests', 0)}
- âœ… ä»£ç è¦†ç›–ç‡: {results.get('coverage', {}).get('line_coverage', 0):.1f}%

### ğŸ“Š è´¨é‡æŒ‡æ ‡
- ğŸ” å‘ç°é—®é¢˜: {len(results.get('static_analysis', {}).get('issues', []))}
- ğŸ› æ€§èƒ½é—®é¢˜: {len(results.get('performance', {}).get('issues', []))}
- ğŸ“ˆ å¤æ‚åº¦é—®é¢˜: {len([i for i in results.get('static_analysis', {}).get('issues', []) if i.get('type') == 'complexity'])}

[æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š](${os.environ.get('GITHUB_SERVER_URL')}/${os.environ.get('GITHUB_REPOSITORY')}/actions/runs/${os.environ.get('GITHUB_RUN_ID')})
"""

# è¿™é‡Œåº”è¯¥è°ƒç”¨GitHub APIåˆ›å»ºè¯„è®ºï¼Œéœ€è¦token
print("PRè¯„è®ºå†…å®¹:")
print(comment)
EOF
            fi
        fi
    else
        echo "ä¸åœ¨CIç¯å¢ƒä¸­ï¼Œè·³è¿‡ä¸Šä¼ "
    fi

else
    echo "é”™è¯¯: åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨"
    exit 1
fi
```

## é…ç½®ç¤ºä¾‹

### 1. Pythoné¡¹ç›®é…ç½®

```yaml
# python-project-config.yaml
project:
  type: python
  language: python
  build_system: setuptools
  test_framework: pytest
  source_directories:
    - "src/"
    - "lib/"
  test_directories:
    - "tests/"
    - "unit_tests/"
    - "integration_tests/"

build:
  enabled: true
  timeout: 300
  commands:
    - "pip install -e ."
    - "python setup.py build"
  artifacts:
    - "dist/"
    - "build/"
    - "*.egg-info/"

test:
  enabled: true
  framework: pytest
  timeout: 600
  coverage_threshold: 85.0
  collect_coverage: true
  commands:
    - "pytest tests/ --cov=src --cov-report=xml --cov-report=html"
    - "pytest integration_tests/ --cov=src --cov-append"
  coverage_formats:
    - "xml"
    - "html"
    - "json"

static_analysis:
  enabled: true
  timeout: 300
  tools:
    python:
      - "pylint --score=no"
      - "flake8"
      - "mypy --strict"
      - "bandit -r"
  complexity_threshold: 8
  security_scan: true

reporting:
  enabled: true
  output_formats:
    - "html"
    - "json"
    - "markdown"
  include_charts: true
  dashboard: true

performance:
  enabled: true
  profile_memory: true
  complexity_analysis: true
  benchmarks:
    enabled: true
    iterations: 10

# è‡ªå®šä¹‰è§„åˆ™
custom_rules:
  enabled: true
  rules_file: "custom_rules.yaml"

# ç¯å¢ƒç‰¹å®šé…ç½®
environments:
  development:
    log_level: "DEBUG"
    skip_coverage: false

  ci:
    log_level: "INFO"
    parallel_jobs: 2
    fail_fast: true

  production:
    log_level: "WARNING"
    skip_build: false
    coverage_threshold: 90.0
```

### 2. å¤šè¯­è¨€é¡¹ç›®é…ç½®

```yaml
# multi-lang-config.yaml
project:
  type: mixed
  languages: ["python", "javascript", "java"]
  build_system: maven  # ä¸»è¦æ„å»ºç³»ç»Ÿ
  test_framework: junit  # ä¸»è¦æµ‹è¯•æ¡†æ¶

# è¯­è¨€ç‰¹å®šé…ç½®
language_configs:
  python:
    source_directories: ["python/src/", "python/lib/"]
    test_directories: ["python/tests/"]
    test_framework: pytest
    static_analysis_tools: ["pylint", "mypy"]

  javascript:
    source_directories: ["frontend/src/", "frontend/lib/"]
    test_directories: ["frontend/tests/", "frontend/__tests__/"]
    test_framework: jest
    static_analysis_tools: ["eslint", "jshint"]

  java:
    source_directories: ["backend/src/main/java/"]
    test_directories: ["backend/src/test/java/"]
    test_framework: junit
    static_analysis_tools: ["spotbugs", "checkstyle"]

# æ„å»ºé…ç½®
build:
  enabled: true
  timeout: 600
  parallel_jobs: 2
  steps:
    - name: "Pythonä¾èµ–å®‰è£…"
      command: "pip install -r python/requirements.txt"
      condition: "exists python/requirements.txt"

    - name: "Node.jsä¾èµ–å®‰è£…"
      command: "cd frontend && npm install"
      condition: "exists frontend/package.json"

    - name: "Javaç¼–è¯‘"
      command: "mvn compile"
      condition: "exists pom.xml"

    - name: "å‰ç«¯æ„å»º"
      command: "cd frontend && npm run build"
      condition: "exists frontend/package.json"

# æµ‹è¯•é…ç½®
test:
  enabled: true
  timeout: 900
  parallel: true
  test_suites:
    - name: "Pythonå•å…ƒæµ‹è¯•"
      command: "pytest python/tests/ --cov=python/src"
      working_directory: "python/"

    - name: "JavaScriptæµ‹è¯•"
      command: "npm test -- --coverage"
      working_directory: "frontend/"

    - name: "Javaæµ‹è¯•"
      command: "mvn test"
      working_directory: "backend/"

# é™æ€åˆ†æé…ç½®
static_analysis:
  enabled: true
  timeout: 600
  parallel: true
  analyze_by_language: true
  language_specific_rules:
    python:
      complexity_threshold: 7
      line_length: 88
    javascript:
      complexity_threshold: 10
      esversion: 2020
    java:
      complexity_threshold: 15
      checkstyle_file: "checkstyle.xml"

# æŠ¥å‘Šé…ç½®
reporting:
  enabled: true
  output_formats:
    - "html"
    - "json"
  group_by_language: true
  comparative_analysis: true
  trend_analysis: true
```

## å·¥ä½œæµç¤ºä¾‹

### 1. å¿«é€Ÿæ£€æŸ¥å·¥ä½œæµ

```yaml
# quick-check-workflow.yaml
name: "å¿«é€Ÿä»£ç è´¨é‡æ£€æŸ¥"
description: "5åˆ†é’Ÿå†…å®Œæˆä»£ç è´¨é‡æ£€æŸ¥"

variables:
  timeout: 300
  parallel: true

steps:
  - name: "ç¯å¢ƒå‡†å¤‡"
    type: "custom"
    command: "echo 'å¼€å§‹å¿«é€Ÿæ£€æŸ¥'"
    timeout: 30

  - name: "è¯­æ³•æ£€æŸ¥"
    type: "analyze"
    config:
      analyzer: "syntax"
      timeout: 120
    dependencies: ["ç¯å¢ƒå‡†å¤‡"]

  - name: "å¿«é€Ÿé™æ€åˆ†æ"
    type: "analyze"
    config:
      analyzer: "static"
      timeout: 180
      skip_expensive: true
    dependencies: ["è¯­æ³•æ£€æŸ¥"]

  - name: "æ ¼å¼æ£€æŸ¥"
    type: "custom"
    command: |
      # Pythonæ ¼å¼æ£€æŸ¥
      find . -name "*.py" -exec black --check {} \;
      find . -name "*.py" -exec isort --check-only {} \;

      # JavaScriptæ ¼å¼æ£€æŸ¥
      find . -name "*.js" -exec eslint --no-eslintrc --config .eslintrc.quick {} \;
    timeout: 60
    dependencies: ["ç¯å¢ƒå‡†å¤‡"]

  - name: "ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š"
    type: "report"
    config:
      format: "markdown"
      template: "quick-report"
      include_summary_only: true
    dependencies: ["è¯­æ³•æ£€æŸ¥", "å¿«é€Ÿé™æ€åˆ†æ", "æ ¼å¼æ£€æŸ¥"]

# å¤±è´¥å¤„ç†
on_failure:
  - name: "é€šçŸ¥å¤±è´¥"
    type: "custom"
    command: "echo 'å¿«é€Ÿæ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š'"
    always_run: true
```

### 2. å®Œæ•´CI/CDå·¥ä½œæµ

```yaml
# complete-cicd-workflow.yaml
name: "å®Œæ•´CI/CDæµæ°´çº¿"
description: "åŒ…å«æ„å»ºã€æµ‹è¯•ã€éƒ¨ç½²çš„å®Œæ•´æµæ°´çº¿"

variables:
  build_parallel: true
  test_parallel: true
  deploy_approval: true

steps:
  # é˜¶æ®µ1: æ„å»ºå‡†å¤‡
  - name: "ä»£ç æ£€å‡º"
    type: "custom"
    command: "echo 'ä»£ç å·²æ£€å‡º'"
    timeout: 60

  - name: "ä¾èµ–å®‰è£…"
    type: "custom"
    command: |
      # å®‰è£…Pythonä¾èµ–
      if [ -f requirements.txt ]; then
        pip install -r requirements.txt
      fi

      # å®‰è£…Node.jsä¾èµ–
      if [ -f package.json ]; then
        npm ci
      fi

      # å®‰è£…Javaä¾èµ–
      if [ -f pom.xml ]; then
        mvn dependency:resolve
      fi
    timeout: 300
    dependencies: ["ä»£ç æ£€å‡º"]

  # é˜¶æ®µ2: æ„å»º
  - name: "é¡¹ç›®æ„å»º"
    type: "build"
    config:
      parallel_jobs: 4
      clean: false
    timeout: 600
    dependencies: ["ä¾èµ–å®‰è£…"]

  - name: "æ„å»ºéªŒè¯"
    type: "custom"
    command: |
      # éªŒè¯æ„å»ºäº§ç‰©
      if [ ! -d "dist" ] && [ ! -d "build" ] && [ ! -d "target" ]; then
        echo "é”™è¯¯: æœªæ‰¾åˆ°æ„å»ºäº§ç‰©"
        exit 1
      fi

      echo "æ„å»ºéªŒè¯é€šè¿‡"
    dependencies: ["é¡¹ç›®æ„å»º"]
    timeout: 60

  # é˜¶æ®µ3: æµ‹è¯•
  - name: "å•å…ƒæµ‹è¯•"
    type: "test"
    config:
      test_type: "unit"
      coverage: true
      parallel: true
    timeout: 900
    dependencies: ["æ„å»ºéªŒè¯"]

  - name: "é›†æˆæµ‹è¯•"
    type: "test"
    config:
      test_type: "integration"
      coverage: false
      parallel: false
    timeout: 1200
    dependencies: ["æ„å»ºéªŒè¯"]

  - name: "æ€§èƒ½æµ‹è¯•"
    type: "custom"
    command: |
      # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
      python scripts/benchmark.py --output performance-results.json

      # æ£€æŸ¥æ€§èƒ½å›å½’
      python scripts/check-performance-regression.py --baseline baseline.json --current performance-results.json
    timeout: 600
    dependencies: ["å•å…ƒæµ‹è¯•"]

  # é˜¶æ®µ4: è´¨é‡æ£€æŸ¥
  - name: "é™æ€ä»£ç åˆ†æ"
    type: "analyze"
    config:
      include_security: true
      include_performance: true
      include_complexity: true
    timeout: 600
    dependencies: ["å•å…ƒæµ‹è¯•"]

  - name: "å®‰å…¨æ‰«æ"
    type: "custom"
    command: |
      # ä¾èµ–å®‰å…¨æ‰«æ
      safety check
      npm audit --audit-level moderate

      # ä»£ç å®‰å…¨æ‰«æ
      bandit -r . -f json -o security-report.json
    timeout: 300
    dependencies: ["ä¾èµ–å®‰è£…"]

  - name: "è®¸å¯è¯æ£€æŸ¥"
    type: "custom"
    command: |
      # æ£€æŸ¥ä¾èµ–è®¸å¯è¯
      pip-licenses --from=mixed --format=json --output-file=licenses.json

      # éªŒè¯è®¸å¯è¯åˆè§„æ€§
      python scripts/check-licenses.py --licenses-file licenses.json
    timeout: 180
    dependencies: ["ä¾èµ–å®‰è£…"]

  # é˜¶æ®µ5: æŠ¥å‘Šç”Ÿæˆ
  - name: "ç”Ÿæˆåˆ†ææŠ¥å‘Š"
    type: "report"
    config:
      formats: ["html", "json", "markdown"]
      include_charts: true
      include_trends: true
    timeout: 300
    dependencies: ["é™æ€ä»£ç åˆ†æ", "å®‰å…¨æ‰«æ", "å•å…ƒæµ‹è¯•", "é›†æˆæµ‹è¯•"]

  - name: "ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"
    type: "custom"
    command: |
      # åˆå¹¶æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
      coverage combine
      coverage html -d test-reports/html
      coverage xml -o test-reports/coverage.xml

      # ç”Ÿæˆæµ‹è¯•è¶‹åŠ¿æŠ¥å‘Š
      python scripts/generate-test-trends.py --output test-trends.json
    timeout: 180
    dependencies: ["å•å…ƒæµ‹è¯•", "é›†æˆæµ‹è¯•"]

  # é˜¶æ®µ6: éƒ¨ç½²å‡†å¤‡
  - name: "æ„å»ºéƒ¨ç½²åŒ…"
    type: "custom"
    command: |
      # åˆ›å»ºéƒ¨ç½²åŒ…
      mkdir -p deployment-package

      # å¤åˆ¶æ„å»ºäº§ç‰©
      cp -r dist/* deployment-package/ 2>/dev/null || true
      cp -r build/* deployment-package/ 2>/dev/null || true
      cp -r target/* deployment-package/ 2>/dev/null || true

      # å¤åˆ¶é…ç½®æ–‡ä»¶
      cp -r config/* deployment-package/ 2>/dev/null || true

      # åˆ›å»ºéƒ¨ç½²å…ƒæ•°æ®
      echo "{
        \"version\": \"$BUILD_VERSION\",
        \"build_time\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
        \"git_commit\": \"$GIT_COMMIT\",
        \"branch\": \"$GIT_BRANCH\"
      }" > deployment-package/deployment-metadata.json

      # æ‰“åŒ…
      tar -czf deployment-package.tar.gz deployment-package/
    timeout: 300
    dependencies: ["ç”Ÿæˆåˆ†ææŠ¥å‘Š", "ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"]

  # é˜¶æ®µ7: éƒ¨ç½²ï¼ˆéœ€è¦å®¡æ‰¹ï¼‰
  - name: "éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ"
    type: "custom"
    command: |
      echo "ç­‰å¾…éƒ¨ç½²å®¡æ‰¹..."

      # éƒ¨ç½²è„šæœ¬
      ./scripts/deploy.sh --environment=staging --package=deployment-package.tar.gz
    timeout: 600
    dependencies: ["æ„å»ºéƒ¨ç½²åŒ…"]
    approval_required: true

  - name: "éƒ¨ç½²åéªŒè¯"
    type: "custom"
    command: |
      # å¥åº·æ£€æŸ¥
      curl -f http://staging.example.com/health || exit 1

      # å†’çƒŸæµ‹è¯•
      python scripts/smoke-tests.py --environment=staging
    timeout: 300
    dependencies: ["éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ"]

  # é˜¶æ®µ8: é€šçŸ¥å’Œæ¸…ç†
  - name: "å‘é€é€šçŸ¥"
    type: "custom"
    command: |
      # å‘é€æˆåŠŸé€šçŸ¥
      curl -X POST "$WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{
          \"text\": \"âœ… CI/CDæµæ°´çº¿æˆåŠŸå®Œæˆ\",
          \"build_number\": \"$BUILD_NUMBER\",
          \"report_url\": \"$REPORT_URL\"
        }"
    timeout: 60
    dependencies: ["éƒ¨ç½²åéªŒè¯"]
    always_run: true

  - name: "æ¸…ç†å·¥ä½œç©ºé—´"
    type: "custom"
    command: |
      # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      rm -rf temp/*
      rm -f *.log

      # æ¸…ç†Dockeré•œåƒï¼ˆå¦‚æœä½¿ç”¨ï¼‰
      docker system prune -f
    timeout: 120
    dependencies: ["å‘é€é€šçŸ¥"]
    always_run: true

# å¤±è´¥å¤„ç†
on_failure:
  - name: "å¤±è´¥é€šçŸ¥"
    type: "custom"
    command: |
      curl -X POST "$WEBHOOK_URL" \
        -H "Content-Type: application/json" \
        -d "{
          \"text\": \"âŒ CI/CDæµæ°´çº¿å¤±è´¥\",
          \"build_number\": \"$BUILD_NUMBER\",
          \"failed_step\": \"$FAILED_STEP\"
        }"
    always_run: true

  - name: "æ”¶é›†æ—¥å¿—"
    type: "custom"
    command: |
      # æ”¶é›†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
      mkdir -p failure-logs
      cp *.log failure-logs/ 2>/dev/null || true

      # æ”¶é›†ç³»ç»Ÿä¿¡æ¯
      uname -a > failure-logs/system-info.txt
      docker version > failure-logs/docker-info.txt 2>/dev/null || true

      # æ‰“åŒ…æ—¥å¿—
      tar -czf failure-logs.tar.gz failure-logs/
    always_run: true
    timeout: 60

# ç¯å¢ƒç‰¹å®šé…ç½®
environments:
  development:
    skip_security_scan: true
    skip_performance_test: true
    deploy_approval: false

  ci:
    parallel_jobs: 4
    fail_fast: true
    timeout_multiplier: 1.0

  production:
    security_scan_level: "strict"
    deploy_approval: true
    rollback_on_failure: true
```

## æ’ä»¶å¼€å‘ç¤ºä¾‹

### 1. è‡ªå®šä¹‰åˆ†æå™¨æ’ä»¶

```python
# custom_analyzer_plugin.py
from ai_dev_system.analyzers.static_analyzer import StaticAnalyzer, CodeIssue
from pathlib import Path
import re
from typing import List

class CustomSecurityAnalyzer:
    """è‡ªå®šä¹‰å®‰å…¨åˆ†æå™¨æ’ä»¶"""

    def __init__(self, config=None):
        self.name = "custom_security_analyzer"
        self.config = config or {}
        self.rules = self._load_security_rules()

    def _load_security_rules(self):
        """åŠ è½½å®‰å…¨è§„åˆ™"""
        return [
            {
                'id': 'hardcoded_password',
                'name': 'ç¡¬ç¼–ç å¯†ç ',
                'pattern': r'(password\s*=\s*["\'][^"\']+["\'])',
                'severity': 'critical',
                'message': 'æ£€æµ‹åˆ°ç¡¬ç¼–ç å¯†ç ',
                'suggestion': 'ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶å­˜å‚¨æ•æ„Ÿä¿¡æ¯',
                'languages': ['python', 'javascript', 'java']
            },
            {
                'id': 'sql_injection_risk',
                'name': 'SQLæ³¨å…¥é£é™©',
                'pattern': r'(execute\s*\(\s*["\'].*\+.*["\']|cursor\.execute\s*\(\s*["\'].*%\s*.*["\'])',
                'severity': 'high',
                'message': 'å¯èƒ½çš„SQLæ³¨å…¥é£é™©',
                'suggestion': 'ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æˆ–ORM',
                'languages': ['python', 'java']
            },
            {
                'id': 'insecure_deserialization',
                'name': 'ä¸å®‰å…¨çš„ååºåˆ—åŒ–',
                'pattern': r'(pickle\.loads|yaml\.load\s*\([^)]*\)(?!\s*Loader\))',
                'severity': 'high',
                'message': 'ä¸å®‰å…¨çš„ååºåˆ—åŒ–æ“ä½œ',
                'suggestion': 'ä½¿ç”¨å®‰å…¨çš„åºåˆ—åŒ–æ ¼å¼æˆ–éªŒè¯è¾“å…¥',
                'languages': ['python']
            }
        ]

    def analyze(self, project_path: Path) -> List[CodeIssue]:
        """åˆ†æé¡¹ç›®å®‰å…¨é—®é¢˜"""
        issues = []

        # è·å–é¡¹ç›®ä¸­çš„ä»£ç æ–‡ä»¶
        code_files = self._get_code_files(project_path)

        for file_path in code_files:
            file_issues = self._analyze_file(file_path)
            issues.extend(file_issues)

        return issues

    def _get_code_files(self, project_path: Path) -> List[Path]:
        """è·å–éœ€è¦åˆ†æçš„ä»£ç æ–‡ä»¶"""
        extensions = ['.py', '.js', '.java', '.ts']
        code_files = []

        for ext in extensions:
            code_files.extend(project_path.rglob(f'*{ext}'))

        # æ’é™¤ä¸€äº›ç›®å½•
        exclude_dirs = {'node_modules', '.git', '__pycache__', 'venv', 'env'}
        code_files = [f for f in code_files
                     if not any(exclude_dir in str(f).split('/') for exclude_dir in exclude_dirs)]

        return code_files

    def _analyze_file(self, file_path: Path) -> List[CodeIssue]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        issues = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ£€æµ‹æ–‡ä»¶è¯­è¨€
            language = self._detect_language(file_path)

            # åº”ç”¨å®‰å…¨è§„åˆ™
            for rule in self.rules:
                if language in rule['languages']:
                    matches = re.finditer(rule['pattern'], content, re.IGNORECASE | re.MULTILINE)

                    for match in matches:
                        # è®¡ç®—è¡Œå·
                        line_number = content[:match.start()].count('\n') + 1

                        issue = CodeIssue(
                            type='security',
                            severity=rule['severity'],
                            file_path=str(file_path.relative_to(file_path.parents[-2])),
                            line_number=line_number,
                            column_number=match.start() - content.rfind('\n', 0, match.start()),
                            message=rule['message'],
                            description=f"å®‰å…¨è§„åˆ™: {rule['name']}",
                            suggestion=rule['suggestion'],
                            rule_id=rule['id'],
                            category='security'
                        )
                        issues.append(issue)

        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

        return issues

    def _detect_language(self, file_path: Path) -> str:
        """æ£€æµ‹æ–‡ä»¶è¯­è¨€"""
        suffix = file_path.suffix.lower()

        language_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.cs': 'csharp'
        }

        return language_map.get(suffix, 'unknown')

# æ’ä»¶æ³¨å†Œå‡½æ•°
def register_plugin(plugin_manager):
    """æ³¨å†Œæ’ä»¶åˆ°æ’ä»¶ç®¡ç†å™¨"""
    plugin_manager.register_analyzer('custom_security', CustomSecurityAnalyzer)

# æ’ä»¶é…ç½®ç¤ºä¾‹
PLUGIN_CONFIG = {
    'custom_security_analyzer': {
        'enabled': True,
        'severity_threshold': 'medium',  # åªæŠ¥å‘ŠmediumåŠä»¥ä¸Šçº§åˆ«çš„é—®é¢˜
        'exclude_patterns': [
            'test_*.py',  # æ’é™¤æµ‹è¯•æ–‡ä»¶
            '*_test.py',
            'tests/*'
        ],
        'custom_rules_file': 'custom_security_rules.yaml'  # è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶
    }
}
```

### 2. è‡ªå®šä¹‰æŠ¥å‘Šæ’ä»¶

```python
# custom_report_plugin.py
from ai_dev_system.reporters.report_generator import ReportGenerator
from pathlib import Path
import json
import jinja2
from datetime import datetime

class CustomMarkdownReporter:
    """è‡ªå®šä¹‰MarkdownæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, config=None):
        self.name = "custom_markdown_reporter"
        self.config = config or {}
        self.template_dir = Path(__file__).parent / 'templates'

    def generate_report(self, data: dict, output_path: Path) -> Path:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        # åŠ è½½æ¨¡æ¿
        template = self._load_template('custom_report.md')

        # å‡†å¤‡æ¨¡æ¿æ•°æ®
        template_data = self._prepare_template_data(data)

        # æ¸²æŸ“æ¨¡æ¿
        content = template.render(**template_data)

        # ä¿å­˜æŠ¥å‘Š
        report_file = output_path / 'custom_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)

        return report_file

    def _load_template(self, template_name: str) -> jinja2.Template:
        """åŠ è½½Jinja2æ¨¡æ¿"""
        template_path = self.template_dir / template_name

        if not template_path.exists():
            # ä½¿ç”¨å†…ç½®æ¨¡æ¿
            template_content = self._get_builtin_template(template_name)
        else:
            with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()

        return jinja2.Template(template_content)

    def _get_builtin_template(self, template_name: str) -> str:
        """è·å–å†…ç½®æ¨¡æ¿"""
        if template_name == 'custom_report.md':
            return """
# {{ project_name }} - AIå¼€å‘ç³»ç»Ÿåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {{ timestamp }}
**åˆ†æç‰ˆæœ¬**: {{ version }}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

{% if build_success %}
- âœ… **æ„å»ºçŠ¶æ€**: æˆåŠŸ
{% else %}
- âŒ **æ„å»ºçŠ¶æ€**: å¤±è´¥
{% endif %}

- ğŸ§ª **æµ‹è¯•é€šè¿‡ç‡**: {{ test_pass_rate }}%
- ğŸ“ˆ **ä»£ç è¦†ç›–ç‡**: {{ coverage_percentage }}%
- ğŸ” **å‘ç°é—®é¢˜æ€»æ•°**: {{ total_issues }}
- ğŸš¨ **ä¸¥é‡é—®é¢˜**: {{ critical_issues }}

## ğŸ—ï¸ æ„å»ºç»“æœ

{% if build_success %}
æ„å»ºæˆåŠŸå®Œæˆï¼Œè€—æ—¶ {{ build_duration }}ç§’ã€‚
{% else %}
æ„å»ºå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š
```
{{ build_error }}
```
{% endif %}

## ğŸ§ª æµ‹è¯•ç»“æœ

- **æ€»æµ‹è¯•æ•°**: {{ total_tests }}
- **é€šè¿‡**: {{ passed_tests }}
- **å¤±è´¥**: {{ failed_tests }}
- **è·³è¿‡**: {{ skipped_tests }}

{% if coverage_percentage > 0 %}
### è¦†ç›–ç‡è¯¦æƒ…
- **è¡Œè¦†ç›–ç‡**: {{ coverage_percentage }}%
- **åˆ†æ”¯è¦†ç›–ç‡**: {{ branch_coverage }}%
- **å‡½æ•°è¦†ç›–ç‡**: {{ function_coverage }}%
{% endif %}

## ğŸ” ä»£ç è´¨é‡åˆ†æ

### é—®é¢˜ç»Ÿè®¡
| ä¸¥é‡ç¨‹åº¦ | æ•°é‡ |
|---------|------|
| ğŸš¨ ä¸¥é‡ | {{ critical_issues }} |
| âš ï¸  é«˜ | {{ high_issues }} |
| ğŸ’¡ ä¸­ç­‰ | {{ medium_issues }} |
| â„¹ï¸  ä½ | {{ low_issues }} |

### é—®é¢˜è¯¦æƒ…

{% for issue in top_issues %}
#### {{ issue.severity }} - {{ issue.message }}
- **æ–‡ä»¶**: `{{ issue.file_path }}:{{ issue.line_number }}`
- **å»ºè®®**: {{ issue.suggestion }}

{% endfor %}

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

{% if trend_data %}
{% for metric in trend_data %}
### {{ metric.name }}
- å½“å‰: {{ metric.current }}
- ä¸Šæ¬¡: {{ metric.previous }}
- å˜åŒ–: {{ metric.change }} ({{ metric.trend }})

{% endfor %}
{% else %}
æš‚æ— å†å²æ•°æ®ç”¨äºè¶‹åŠ¿åˆ†æã€‚
{% endif %}

## ğŸ”§ å»ºè®®ä¿®å¤

{% for fix in suggested_fixes %}
1. **{{ fix.title }}**
   - ä¼˜å…ˆçº§: {{ fix.priority }}
   - é¢„ä¼°å·¥ä½œé‡: {{ fix.effort }}
   - æè¿°: {{ fix.description }}

{% endfor %}

---

*æ­¤æŠ¥å‘Šç”±AIå¼€å‘ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
            """
        else:
            return "# æ¨¡æ¿æœªæ‰¾åˆ°"

    def _prepare_template_data(self, data: dict) -> dict:
        """å‡†å¤‡æ¨¡æ¿æ•°æ®"""
        project_info = data.get('project', {})
        build_result = data.get('build', {})
        test_result = data.get('test', {})
        coverage_data = data.get('coverage', {})
        static_analysis = data.get('static_analysis', {})

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_tests = test_result.get('total_tests', 0)
        passed_tests = test_result.get('passed_tests', 0)
        test_pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        coverage_percentage = coverage_data.get('line_coverage', 0)

        issues = static_analysis.get('issues', [])
        issues_by_severity = {}
        for issue in issues:
            severity = issue.get('severity', 'low')
            issues_by_severity[severity] = issues_by_severity.get(severity, 0) + 1

        # è·å–å‰10ä¸ªé—®é¢˜
        top_issues = sorted(issues, key=lambda x: self._severity_weight(x.get('severity', 'low')), reverse=True)[:10]

        return {
            'project_name': project_info.get('name', 'Unknown'),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'version': data.get('version', '1.0.0'),

            'build_success': build_result.get('success', False),
            'build_duration': build_result.get('duration', 0),
            'build_error': build_result.get('error_output', '')[:500],

            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': test_result.get('failed_tests', 0),
            'skipped_tests': test_result.get('skipped_tests', 0),
            'test_pass_rate': f"{test_pass_rate:.1f}",

            'coverage_percentage': f"{coverage_percentage:.1f}",
            'branch_coverage': f"{coverage_data.get('branch_coverage', 0):.1f}",
            'function_coverage': f"{coverage_data.get('function_coverage', 0):.1f}",

            'total_issues': len(issues),
            'critical_issues': issues_by_severity.get('critical', 0),
            'high_issues': issues_by_severity.get('high', 0),
            'medium_issues': issues_by_severity.get('medium', 0),
            'low_issues': issues_by_severity.get('low', 0),

            'top_issues': top_issues,

            'trend_data': data.get('trends', []),
            'suggested_fixes': self._generate_suggested_fixes(issues)
        }

    def _severity_weight(self, severity: str) -> int:
        """è·å–ä¸¥é‡ç¨‹åº¦æƒé‡"""
        weights = {
            'critical': 4,
            'high': 3,
            'medium': 2,
            'low': 1
        }
        return weights.get(severity, 0)

    def _generate_suggested_fixes(self, issues: list) -> list:
        """ç”Ÿæˆå»ºè®®ä¿®å¤åˆ—è¡¨"""
        fixes = []

        # æŒ‰ä¸¥é‡ç¨‹åº¦å’Œç±»å‹åˆ†ç»„é—®é¢˜
        issues_by_type = {}
        for issue in issues:
            issue_type = issue.get('type', 'other')
            if issue_type not in issues_by_type:
                issues_by_type[issue_type] = []
            issues_by_type[issue_type].append(issue)

        # ä¸ºæ¯ç§ç±»å‹ç”Ÿæˆä¿®å¤å»ºè®®
        for issue_type, type_issues in issues_by_type.items():
            if issue_type == 'security':
                fixes.append({
                    'title': 'ä¿®å¤å®‰å…¨é—®é¢˜',
                    'priority': 'é«˜',
                    'effort': '2-4å°æ—¶',
                    'description': f'å‘ç° {len(type_issues)} ä¸ªå®‰å…¨é—®é¢˜ï¼Œå»ºè®®ç«‹å³ä¿®å¤ç¡¬ç¼–ç å¯†ç ã€SQLæ³¨å…¥ç­‰å®‰å…¨æ¼æ´ã€‚'
                })
            elif issue_type == 'complexity':
                fixes.append({
                    'title': 'é™ä½ä»£ç å¤æ‚åº¦',
                    'priority': 'ä¸­',
                    'effort': '4-8å°æ—¶',
                    'description': f'å‘ç° {len(type_issues)} ä¸ªå¤æ‚åº¦è¿‡é«˜çš„å‡½æ•°ï¼Œå»ºè®®é‡æ„ä¸ºæ›´å°çš„å‡½æ•°ã€‚'
                })
            elif issue_type == 'style':
                fixes.append({
                    'title': 'æ”¹è¿›ä»£ç é£æ ¼',
                    'priority': 'ä½',
                    'effort': '1-2å°æ—¶',
                    'description': f'å‘ç° {len(type_issues)} ä¸ªä»£ç é£æ ¼é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨è‡ªåŠ¨æ ¼å¼åŒ–å·¥å…·ä¿®å¤ã€‚'
                })

        return fixes

# æ’ä»¶æ³¨å†Œå‡½æ•°
def register_plugin(plugin_manager):
    """æ³¨å†Œæ’ä»¶åˆ°æ’ä»¶ç®¡ç†å™¨"""
    plugin_manager.register_reporter('custom_markdown', CustomMarkdownReporter)
```

### 3. æ’ä»¶ä½¿ç”¨ç¤ºä¾‹

```python
# plugin_usage_example.py
from ai_dev_system import MasterController, ConfigManager
from pathlib import Path

# åŠ è½½è‡ªå®šä¹‰æ’ä»¶
from custom_analyzer_plugin import CustomSecurityAnalyzer
from custom_report_plugin import CustomMarkdownReporter

class PluginManager:
    """æ’ä»¶ç®¡ç†å™¨"""
    def __init__(self):
        self.analyzers = {}
        self.reporters = {}

    def register_analyzer(self, name, analyzer_class):
        """æ³¨å†Œåˆ†æå™¨æ’ä»¶"""
        self.analyzers[name] = analyzer_class

    def register_reporter(self, name, reporter_class):
        """æ³¨å†ŒæŠ¥å‘Šå™¨æ’ä»¶"""
        self.reporters[name] = reporter_class

    def get_analyzer(self, name, config=None):
        """è·å–åˆ†æå™¨å®ä¾‹"""
        if name in self.analyzers:
            return self.analyzers[name](config)
        return None

    def get_reporter(self, name, config=None):
        """è·å–æŠ¥å‘Šå™¨å®ä¾‹"""
        if name in self.reporters:
            return self.reporters[name](config)
        return None

# ä½¿ç”¨è‡ªå®šä¹‰æ’ä»¶
def main():
    # åˆ›å»ºæ’ä»¶ç®¡ç†å™¨
    plugin_manager = PluginManager()

    # æ³¨å†Œæ’ä»¶
    plugin_manager.register_analyzer('custom_security', CustomSecurityAnalyzer)
    plugin_manager.register_reporter('custom_markdown', CustomMarkdownReporter)

    # åˆ›å»ºé…ç½®
    config = ConfigManager()

    # è·å–è‡ªå®šä¹‰åˆ†æå™¨
    security_analyzer = plugin_manager.get_analyzer('custom_security')
    if security_analyzer:
        project_path = Path("./my-project")
        security_issues = security_analyzer.analyze(project_path)
        print(f"å‘ç° {len(security_issues)} ä¸ªå®‰å…¨é—®é¢˜")

    # è·å–è‡ªå®šä¹‰æŠ¥å‘Šå™¨
    markdown_reporter = plugin_manager.get_reporter('custom_markdown')
    if markdown_reporter:
        # æ¨¡æ‹Ÿåˆ†ææ•°æ®
        analysis_data = {
            'project': {'name': 'My Project'},
            'build': {'success': True, 'duration': 120},
            'test': {'total_tests': 100, 'passed_tests': 95, 'failed_tests': 3, 'skipped_tests': 2},
            'coverage': {'line_coverage': 85.5, 'branch_coverage': 78.2, 'function_coverage': 92.1},
            'static_analysis': {'issues': security_issues}
        }

        output_path = Path("./custom-reports")
        output_path.mkdir(exist_ok=True)

        report_file = markdown_reporter.generate_report(analysis_data, output_path)
        print(f"è‡ªå®šä¹‰æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

if __name__ == '__main__':
    main()
```

---

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†AI Development Systemçš„å„ç§ä½¿ç”¨æ–¹å¼ï¼Œä»åŸºç¡€çš„å‘½ä»¤è¡Œæ“ä½œåˆ°é«˜çº§çš„æ’ä»¶å¼€å‘ã€‚æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„ç¤ºä¾‹å¹¶è¿›è¡Œç›¸åº”çš„è°ƒæ•´ã€‚